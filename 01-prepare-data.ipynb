{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "\n",
    "In this script we will prepare the data for the DeepLabeler model\n",
    "\n",
    "Author: Ryan Fogle\n",
    "\n",
    "Date: 4-7-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from datetime import datetime\n",
    "start = datetime.now()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Notes & ICD9 Codes\n",
    "\n",
    "We will now load in the notes from the MIMIC-III dataset for the discharge summaries and then join this dataset with the ICD9 codes. \n",
    "\n",
    "To get access to the MIMIC-III dataset please follow the guidelines as seen in this link: https://eicu-crd.mit.edu/gettingstarted/access/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! change this to the location on your machine!\n",
    "data_url = '../physionet.org/files/mimiciii/1.4/'\n",
    "\n",
    "notes = pd.read_csv(data_url + 'NOTEEVENTS.csv')\n",
    "notes.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset to only discharge summaries\n",
    "\n",
    "As you will see, notes has many different types of categories. We will want to grab only the Discharge summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharges = notes[notes['CATEGORY'] == 'Discharge summary'].copy()\n",
    "discharges.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate multiple discharge summaries\n",
    "\n",
    "About 21% of the notes population has more than 1 discharge summary for a visit, we will combine these notes to get only 1 discharge summary per visit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = discharges.groupby(['SUBJECT_ID', 'HADM_ID'])['ROW_ID'].count()\n",
    "f\"Percent of pop with more than 1 discharge summary for one visit: {gb[gb > 1].sum() / discharges.shape[0] * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find text length and then sort dataframe by char length, we want the most informative notes to show first due to needing to truncate the data later\n",
    "discharges['TEXT_LEN'] = discharges['TEXT'].progress_apply(lambda x: len(x))\n",
    "discharges.sort_values('TEXT_LEN', ascending=False, inplace=True)\n",
    "\n",
    "discharges = discharges[['SUBJECT_ID', 'HADM_ID', 'TEXT']].drop_duplicates().groupby(['SUBJECT_ID', 'HADM_ID'])['TEXT'].progress_apply(lambda x: \" \".join(x)).reset_index()\n",
    "discharges.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(discharges.iloc[np.random.randint(0, discharges.shape[0])]['TEXT'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary tokens\n",
    "\n",
    "The paper did not mention how they tokenized the text, so we will need to create our own tokenization process. You will notice that some information from the notes is omitted, like the name of the patient, the dates, the doctor name, and the hospital's name. \n",
    "\n",
    "This information will not be useful for our purposes, logically speaking the name of the patient, doctor, hospital, or dates by themselves should not be indicators of what ICD9 codes will be diagnosed. These will be considered stop words for the tokenization process.\n",
    "\n",
    "Secondly, we will lowercase the words - this will decrease our vocab size. \n",
    "\n",
    "Third, we will remove all punctuation and extra white space from the text. This is done for the same reason, to reduce our vocab size and size of our notes. Punction would be considered stop words for the SVM model and we want to only consider words for our Word2Vec and Doc2Vec models. This approach will have problems, it will treat e.coli as \"e\" and \"coli\" when we perhaps would want to treat it as one token. \n",
    "\n",
    "Fourth, we will remove all numbers. Individual numbers themselves should not present a signficant advantage for word2vec, as many numbers are measurements from the patient. We will strictly remove timestamps as well (ie 10:01PM)\n",
    "\n",
    "Creating a better tokenizer could be the next steps of this project, but I opted for a simpiler regex solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "discharges['TEXT'] = discharges['TEXT'].progress_apply(lambda x: x.lower())\n",
    "\n",
    "# remove #1, #3\n",
    "p = re.compile(\"(\\[\\*\\*.+?\\*\\*\\])|([0-9]{1,2}:[0-9]{2}([AaPp][Mm]){0,1})|([!\\\"#$%&\\'()*+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\")\n",
    "s = re.compile(\"\\s+\") # replace excessive white space with one space\n",
    "n = re.compile('[0-9]+') # replace numbers\n",
    "w = re.compile('(admission\\sdate)|(discharge\\sdate)|(date\\sof\\sbirth)|(pm)|(am)|(mg)') # remove common words\n",
    "discharges['TEXT'] = discharges['TEXT'].progress_apply(lambda x: p.sub(' ', x)).progress_apply(lambda x: n.sub('', x)).progress_apply(lambda x: w.sub('', x)).progress_apply(lambda x: s.sub(' ', x))\n",
    "discharges['TEXT'].iloc[100,]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharges['toks'] = discharges['TEXT'].progress_apply(lambda x: x.split())\n",
    "toks_len = discharges['toks'].progress_apply(lambda x: len(x))\n",
    "toks_len.agg(['mean', 'median', 'std', 'max', 'min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save if needed\n",
    "discharges.to_parquet('discharges.pq')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizer is similar enough with the counts listed in the paper, we will stop the tokenization process here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in ICD9 Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_raw = pd.read_csv(data_url + 'DIAGNOSES_ICD.csv')\n",
    "diag = diag_raw[diag_raw['ICD9_CODE'].notna()].copy()\n",
    "diag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = diag.groupby(['SUBJECT_ID', 'HADM_ID'])['ICD9_CODE'].progress_apply(lambda x: list(x)).reset_index()\n",
    "diag.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Occurances of ICD9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(diag['ICD9_CODE'].to_list())\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.Series(dict(zip(mlb.classes_, np.sum(y, axis=0))))\n",
    "ax = codes.hist(bins=np.arange(0,100, 1))\n",
    "ax.set_xlabel('Occurrence Count')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see many codes have 20 or less examples, to speed up training we will ignore codes that occur less than 50 times. \n",
    "\n",
    "The authors of the DeepLabeler trained a model to handle labeling all codes, but when there is less than 50 positive cases you are going to have a very small training set. We will now remove all codes that occur less than 50 times, when we do a test/train split of 80/20 we will have on average 10 test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_codes = codes[codes > 50].index.to_list()\n",
    "len(valid_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_codes = codes[codes > 50].index.to_list()\n",
    "diag = diag_raw[diag_raw['ICD9_CODE'].notna() & diag_raw['ICD9_CODE'].isin(valid_codes)]\n",
    "diag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = diag.groupby(['SUBJECT_ID', 'HADM_ID'])['ICD9_CODE'].progress_apply(lambda x: list(x)).reset_index()\n",
    "diag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = discharges.merge(diag, left_on=['SUBJECT_ID', 'HADM_ID'], right_on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "print(final.shape)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_parquet('prepared-data.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.toks.str.len().hist(bins= np.arange(0, 6000, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "total_time = end - start\n",
    "total_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d716bf7a04a4236457ac4904b85c3b09395e2ba295a5d1cc0de156cec0f44305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
